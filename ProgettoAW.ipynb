{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProgettoAW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkPHZacEqZLj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5cebc220-2ecd-4a7a-864b-a342bd00190b"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import tensorflow as tf\n",
        "import os\n",
        "import glob\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import math\n",
        "import datetime\n",
        "\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.nasnet import NASNetMobile, NASNetLarge\n",
        "from keras.layers import Activation, Dense, Input, Dropout, MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D, GaussianDropout, BatchNormalization, Flatten\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam, RMSprop, sgd\n",
        "\n",
        "np.random.seed(seed=9876)\n",
        "\n",
        "INPUT_SIZE = 331 # size dell'input della rete\n",
        "nb_classes = 35 # numero di classi \n",
        "rows = 2205 # numero di elementi del dataset\n",
        "bs = 128 #BATCH_SIZE\n",
        "nb_epoch = 5 # numero di epoche"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oant-9HEi_am",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(username,key):\n",
        "  #funzione per scaricare i dati da kaggle\n",
        "  os.environ['KAGGLE_USERNAME'] = username\n",
        "  os.environ['KAGGLE_KEY'] = key\n",
        "  !kaggle competitions download -c aw18-19\n",
        "  !ls /content\n",
        "  !unzip Dataset.zip\n",
        "  !ls /content/Dataset/train\n",
        "  \n",
        "def isTrain():\n",
        "  rnd= np.random.random()\n",
        "  if rnd<0.80:\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "    \n",
        "def data_split():\n",
        "  # crea una cartella validation, con all'interno una cartella per razza.\n",
        "  # per ogni razza si spostano il 20% delle immagini nella corrispondente cartella del validation.\n",
        "  os.chdir('/content/Dataset')\n",
        "  !ls\n",
        "  !rm -rf validation\n",
        "  !rm -rf models\n",
        "  \n",
        "  os.mkdir('models')\n",
        "  os.chdir('/content/Dataset')\n",
        "  os.mkdir('validation')\n",
        "\n",
        "  for path in os.listdir('/content/Dataset/train'):\n",
        "    os.chdir('/content/Dataset/validation')\n",
        "    os.mkdir(path)\n",
        "    os.chdir('/content')\n",
        "    for filename in glob.glob('/content/Dataset/train/'+path+'/*.jpg'):\n",
        "        if not isTrain() :\n",
        "          completePath = filename.split('/')\n",
        "          pathToCopy = \"/content/Dataset/validation/\"+path+\"/\"+completePath[5]\n",
        "          shutil.move(filename, pathToCopy) \n",
        "  \n",
        "  \n",
        "def createModelNASNetLarge():\n",
        "  # si prende il modello NASNetLarge tramite keras\n",
        "  base_model = NASNetLarge(weights = 'imagenet', \n",
        "\tinclude_top = False, \n",
        "\tinput_shape=(INPUT_SIZE, INPUT_SIZE, 3),\n",
        "  pooling='avg')\n",
        "\t\t  \n",
        "  x = base_model.output\n",
        "  x = GaussianDropout(0.2)(x)\n",
        "  \n",
        "  x = Dense(nb_classes*4, activation='relu')(x) \n",
        "  x = GaussianDropout(0.2)(x)\n",
        "  \n",
        "  x = Dense(nb_classes*2, activation='relu')(x) \n",
        "  x = GaussianDropout(0.2)(x)\n",
        "  \n",
        "  predictions = Dense(nb_classes, activation='softmax')(x)\n",
        "  model = Model(inputs = base_model.input, outputs = predictions)\n",
        "\n",
        "  # non è necessario riaddestrare i layer del modello base\n",
        "  for layer in base_model.layers:\n",
        "\t   layer.trainable = False\n",
        "      \n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "  return(model)\n",
        "\n",
        "def train(model):\n",
        "  #Si imposta l'erly-stopping: si monitora il valore dell'accuracy sul validation, se in un epoca successiva diminuisce, viene terminato l'addestramento\n",
        "  early_stopping = EarlyStopping(monitor='val_acc', mode='max')\n",
        "\n",
        "  history = model.fit_generator(train_generator,\n",
        "                             steps_per_epoch=math.ceil(rows/bs),\n",
        "                             epochs=nb_epoch, \n",
        "                             callbacks=[early_stopping],\n",
        "                             validation_data= validation_generator,\n",
        "                             validation_steps=math.ceil(rows/bs))\n",
        "\n",
        "  print(history.history.keys())\n",
        "  # grafici dell'accuracy epoca per epoca\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  # grafici della loss epoca per epoca\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "def predictions():\n",
        "  t_images =[]\n",
        "  filenames = []\n",
        "  pathTest= '/content/Dataset/test'\n",
        "  for filename in os.listdir(pathTest):\n",
        "     img = image.load_img(pathTest+'/'+filename, target_size=(INPUT_SIZE,INPUT_SIZE) )\n",
        "     img = image.img_to_array(img)\n",
        "     img/=255\n",
        "     t_images.append(img)\n",
        "     fil = filename.split('.')\n",
        "     filenames.append(fil[0])\n",
        "\n",
        "  x_test1 = np.array(t_images)\n",
        "  pred = model.predict(x_test1)\n",
        "  return pred\n",
        "\n",
        "def createOutput(pred):\n",
        "  array = []\n",
        "  breed = sorted(os.listdir('/content/Dataset/train'))\n",
        "  for i in range(len(pred)):\n",
        "    array.append(np.argmax(pred[i]))\n",
        "  dictionary = {'id':'breed'}\n",
        "  for i in range(len(pred)):\n",
        "    dictionary[filenames[i]]= breed[array[i]] \n",
        "  # si scrive il file delle prevision in csv secondo il formato \"id,\"breed\"\n",
        "  with open('result.csv','w') as f:\n",
        "    for k in dictionary.keys():\n",
        "      f.write(\"%s,%s\\n\"%(k,dictionary[k]))\n",
        "      \n",
        "  #per scaricare il file appena salvato basta andare nell'elenco dei file presenti nella macchina virtuale"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1QhamMrRaH4",
        "colab_type": "text"
      },
      "source": [
        "**LOAD DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ_-IojrOfso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#è necessario fornire l'username e la key per potersi collegare a kaggle con il proprio account\n",
        "username = \"your-kaggle-name\"\n",
        "key = \"your-kaggle-token\"\n",
        "load_data(username,key)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBTgyWWmVUU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_split()\n",
        "# preprocess (sia training che validation) e data Augmentation (solo training)\n",
        "train_datagen = image.ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=15,\n",
        "        width_shift_range=.15,\n",
        "        height_shift_range=.15,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# si leggono le immagini dalle relative cartelle a batch per non occupare tutta la ram disponibile\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/Dataset/train',\n",
        "        target_size=(INPUT_SIZE, INPUT_SIZE),\n",
        "        batch_size= bs,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True,\n",
        "        seed=1234)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        '/content/Dataset/validation',\n",
        "        target_size=(INPUT_SIZE, INPUT_SIZE),\n",
        "        batch_size=bs,\n",
        "        class_mode='categorical', \n",
        "        shuffle=True,\n",
        "        seed=1234)\n",
        "\n",
        "model = createModelNASNetLarge()\n",
        "train(model)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN_o0eP4UlNx",
        "colab_type": "text"
      },
      "source": [
        "**PREDICT** & **CREATE OUTPUT FILE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zj276PCw0nt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = predictions()\n",
        "createOutput(pred)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
